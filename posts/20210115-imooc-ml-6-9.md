# 6-9 有关梯度下降的更多深入讨论

- 批量梯度下降法（Batch Gradient Descent）
- 随机梯度下降法（Stochastic Gradient Descent）
- 小批量梯度下降法（Mini-Batch Gradient Descent）：综合上两者的优点
  - 超参数：batch_size

## 随机

- 跳出局部最优解
- 更快的运行速度
- 机器学习领域很多算法都要使用随机的特征
  - 随机搜索；随机森林

## 梯度上升法

- 梯度可以代表方向，对应 J 增大的最快方向
  - 梯度下降：$\theta = \theta - \eta \frac{\partial J}{\partial \theta}$
  - 梯度上升：$\theta = \theta + \eta \frac{\partial J}{\partial \theta}$
- 梯度上升：最大化一个效用函数。

