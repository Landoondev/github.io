# 5-9 使用 sklearn 解决回归问题：LinearRegression、KNeighborsRegressor

## sklearn 中的回归问题


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
```


```python
boston = datasets.load_boston()

X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]
```


```python
from playML.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, seed=666)
```


```python
from playML.LinearRegression import LinearRegression

reg = LinearRegression()
reg.fit_normal(X_train, y_train)
```




    LinearRegression




```python
reg.coef_
```




    array([-1.20354261e-01,  3.64423279e-02, -3.61493155e-02,  5.12978140e-02,
           -1.15775825e+01,  3.42740062e+00, -2.32311760e-02, -1.19487594e+00,
            2.60101728e-01, -1.40219119e-02, -8.35430488e-01,  7.80472852e-03,
           -3.80923751e-01])




```python
reg.interception_
```




    34.11739972320428




```python

```

## sklearn 中的线性回归


```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
```


```python
lin_reg.fit(X_train, y_train)
```




    LinearRegression()




```python
# 系数
lin_reg.coef_
```




    array([-1.20354261e-01,  3.64423279e-02, -3.61493155e-02,  5.12978140e-02,
           -1.15775825e+01,  3.42740062e+00, -2.32311760e-02, -1.19487594e+00,
            2.60101728e-01, -1.40219119e-02, -8.35430488e-01,  7.80472852e-03,
           -3.80923751e-01])




```python
# 截距
lin_reg.intercept_
```




    34.117399723229546




```python
lin_reg.score(X_test, y_test)
```




    0.8129794056212811



## kNN 解决回归问题


```python
from sklearn.neighbors import KNeighborsRegressor # 解决回归问题

knn_reg = KNeighborsRegressor()
knn_reg.fit(X_train, y_train)

knn_reg.score(X_test, y_test)
```




    0.5865412198300899




```python
# 超参数搜索
from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        "weights": ["uniform"],
        "n_neighbors": [i for i in range(1, 11)]
    },
    {
        "weights": ["distance"],
        "n_neighbors": [i for i in range(1, 11)],
        "p": [i for i in range(1, 6)]
    }
]
```


```python
knn_reg = KNeighborsRegressor()
grid_search = GridSearchCV(knn_reg, param_grid, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)
```

    Fitting 5 folds for each of 60 candidates, totalling 300 fits


    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
    [Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    4.3s
    [Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    4.6s finished





    GridSearchCV(estimator=KNeighborsRegressor(), n_jobs=-1,
                 param_grid=[{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                              'weights': ['uniform']},
                             {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                              'p': [1, 2, 3, 4, 5], 'weights': ['distance']}],
                 verbose=1)




```python
# 搜索到的最好结果
grid_search.best_estimator_
```




    KNeighborsRegressor(n_neighbors=7, p=1, weights='distance')




```python
grid_search.best_score_
```




    0.652216494152461




```python
grid_search.best_estimator_.score(X_test, y_test)
```




    0.7160666820548707



从 0.5865412198300899 提升到了 0.5865412198300899。但是表现仍不如线性回归。

