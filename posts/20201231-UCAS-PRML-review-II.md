# 国科大 PRML 复习（2）

> PRML 随堂测

## 判别式模型/生成式模型

**❓1、什么是判别式模型？什么是生成式模型？**

假设可观测到的变量集合为 $X$，需要预测的变量集合为 $Y$，其他的变量集合为 $Z$。

- **判别式模型**直接对条件概率分布 $P(Y,Z \mid X)$ 进行建模，然后消掉无关变量 $Z$ 就可以得到对变量集合 $Y$ 的预测，即：
  - $$P(Y \mid X) = \sum_{Z} P(Y, Z \mid X)$$
- 生成式模型是对联合概率分布 $P(X,Y,Z)$ 进行建模，在给定观测集合 $X$ 的条件下，通过计算边缘分布来得到对变量集合 $Y$ 的预测，即：
  - $$P(Y \mid X) = \frac{P(X,Y)}{P(X)} = \frac{\sum_{Z}P(X,Y,Z)}{\sum_{Y,Z} P(X,Y,Z)}$$

常见的生成式模型：

- 朴素贝叶斯、贝叶斯网络、pLSA（Probabilitic Latent Semantic Analysis，概率隐形语义分析）、LDA（Latent Dirichlet Allocation，隐狄利克雷分布）都是先对联合概率分布进行建模，然后再通过计算边缘分布得到对变量的预测。
- 隐马尔科夫模型

常见的判别式模型：

- 最大熵：直接对条件概率分布进行建模。
- 条件随机场

## SVM 的核函数

**❓在 SVM 中使用具有高 $\gamma$（核函数宽度的倒数）值的 RBF 核函数。意味着什么❓**

Radial Basic Function， RBF 径向基函数

在 SVM 中，平方指数核函数也叫高斯核函数或径向基函数。

- $$k(x_i, x_j)=exp(\frac{-\left\Vert x_i - x_j \right\Vert ^2} {2l^2})$$
  - $l$ 为超参数
  - 当 $x_i $ 和 $x_j$ 越接近，其函数值越大，表面 $f(x_i)$ 和 $f(x_j)$ 越相关

$\gamma$ ：RBF 核宽度的倒数

- $\gamma$ 越大，RBF 核宽度越小，每个训练样本/支持向量的范围越小，模型越复杂。

$C$ ：衡量样本的重要程度

- $C$ 越大，越看重训练样本，模型越复杂。（$C$ 越小模型越简单平滑）

**❓SVM 中的参数 $C$？**

Soft Margin SVM（L1 正则）

$$min \frac{1}{2} \left\Vert w \right\Vert ^2 + C\sum_{i=1}^m \xi_{i}$$

$$s.t. y^{(i)} (w^T x^{(i)} + b) \geq 1- \xi_{i}$$

$$\xi_i \geq 0$$

$$C \uparrow \Rightarrow \xi \downarrow \Rightarrow (1-\xi) \uparrow \Rightarrow 容错能力 \downarrow \Rightarrow Overfitting$$

记住：$\xi = 0$ 就是 Hard Margin。

## n贝叶斯分类

贝叶斯分类：观察值为 $x$，$y$ 有正常（$y=1$） 和异常（$y=0$）两种状态。

- $$P(y \mid x) = \frac{P(y)P(x \mid y)}{P(x)}$$

❓如何求分母 $P(x)$？

## Fisher 线性判别方法

> 简述 Fisher 线性判别方法的基本思路，写出准则函数及对应的解。❓

基本思路：

- 考虑把 $d$ 维空间的样本投影到一条直线上（把特征压缩成一维）。
- 如何找到这个最好的直线方向以及如何实现向最好方向投影的变换，这就是 Fisher 算法要解决的基本问题。
- 投影变换即为解向量 $W^*$。

**Fisher 的准则函数：**

$$J_F(W) = \frac{W^T S_b W}{W^T S_w W}$$

- $S_b$ 为样本类间离散度矩阵（between-class scatter matrix）
- $S_w$ 为总样本类内离散度矩阵（Within-class scatter matrix）

使得 $J_F(W)$ 取得最大值得 $W^*$ 为：

$$W^* = S_{w}^{-1} (m_1 - m_2)$$

- $m_1,m_2$ 分别为 $w_1, w_2$ 类的均值

将训练样本进行投影：

$$y = (W^*)^T X$$

⚠️ 熟悉 $S_w, S_b$ 的计算方法。

## ⚠️ 使用 K-L 变换和 Fisher 线性判别降维

**K-L 变换需要计算：**

- 协方差矩阵（需要考虑先验概率）
- 特征值/特征向量

**Fisher 计算**

- $S_w$
- $S_b$

> 这里我对公式有些疑惑，还加上先验概率的情况，感觉就更加复杂了。

## ✅ 彻底搞懂感知器

### 1. 二分类情形

>  $$w_1: \{ (0,0,0)^T, (1,0,0)^T, (1,0,1)^T, (1,1,0)^T \}$$ 
>
>  $$w_2: \{ (0,0,1)^T, (0,1,1)^T, (0,1,0)^T, (1,1,1)^T \}$$

感知器算法的损失函数：

$$L(w;x,y) =max(0, -yw^Tx)$$

采用随机梯度下降：

$$\frac{\partial L(w;x,y)}{\partial w} = \begin{cases}0& if &yw^Tx > 0 \\ -yx& if & yw^Tx \leq 0\end{cases}$$

参数更新：$w = w - (-yx)$

---

综上，在使用感知器手动迭代时：

**（1）先写出所有样本的增广形式**

$$w_1: \{ (0,0,0,1)^T, (1,0,0,1)^T, (1,0,1,1)^T, (1,1,0,1)^T \}$$

$$w_2: \{ (0,0,1,1)^T, (0,1,1,1)^T, (0,1,0,1)^T, (1,1,1,1)^T \}$$

**（2）$w_1$ 样本乘以 1（$w_1$ 对应的类别为 $y = 1$），$w_2$ 样本乘以 -1（$w_2$ 对应的类别为 $y=-1$）**

$$w_1: \{ (0,0,0,1)^T, (1,0,0,1)^T, (1,0,1,1)^T, (1,1,0,1)^T \}$$

$$w_2: \{ (0,0,-1,-1)^T, (0,-1,-1,-1)^T, (0,-1,0,-1)^T, (-1,-1,-1,-1)^T \}$$

**（3）感知器的损失函数**

$$L(W,x) = max(0, -W^T x)$$

**（4）梯度下降梯度**

$$\frac{\partial L(W,x)}{\partial W} = \begin{cases}0& if &W^Tx > 0 \\ -x& if & W^Tx \leq 0\end{cases}$$

**（5）更新参数**

$$W = W - \alpha(\frac{\partial L(W,x)}{\partial W}) \Rightarrow W = W -\alpha(-x) \Rightarrow W = W + \alpha x \Rightarrow W = W + x$$

- $\alpha$ 为学习率，感知器迭代时，一般取为 1.

### 2. 多分类情形

> $$w_1: (-1,-1)^T$$
>
> $$w_2: (0,0)^T$$
>
> $$w_3: (1,1)^T$$

多分类的情况，需要多个判别函数，因此权向量有多个。对于如上的 3 分类例子，需要计算 $W_1^T, W_2^T, W_3^T$，然后根据权向量的到判别函数 $d_1(x), d_2(x), d_3(x)$。

然后由于这个是多类问题情况 3。

$$w_1 \in \begin{cases}d_1(x) - d_2(x) >0  \\ d_1(x) - d_3(x) > 0& \end{cases}$$

$$w_2 \in \begin{cases}d_2(x) - d_1(x) >0  \\ d_2(x) - d_3(x) > 0& \end{cases}$$

$$w_3 \in \begin{cases}d_3(x) - d_1(x) >0  \\ d_3(x) - d_2(x) > 0& \end{cases}$$

---

**（1）先写出所有样本的增广形式**

$$w_1: (-1,-1,1)^T, w_2:(0,0,1)^T, w_3 : (1,1,1)^T$$

（2）计算 $W^Tx$

❓$$L(W,x) = $$

- (i) $W_1^T \cdot x$
- (ii) $W_2^T \cdot x$
- (iii) $W_3^T \cdot x$

（3）更新规则：

- 如果 $x \in w_1$，且 (i) 的结果为最大值，不更新。
  - (i) 不是最大值，更新策略：
    - $W_1(2) = W_1(1) + x$
    - $W_2(2) = W_2(1) - x$
    - $W_3(2) = W_3(1) - x$
- 如果 $x \in w_2$，且 (ii) 的结果为最大值，不更新。
  - (ii) 不是最大值，更新策略：
    - $W_1(2) = W_1(1) - x$
    - $W_2(2) = W_2(1) + x$
    - $W_3(2) = W_3(1) - x$
- 如果 $x \in w_3$，且 (iii) 的结果为最大值，不更新。
  - (iii) 不是最大值，更新策略：
    - $W_1(2) = W_1(1) - x$
    - $W_2(2) = W_2(1) - x$
    - $W_3(2) = W_3(1) + x$

> ❓（1）为什么有的 $+x$ 而有的 $-x$，可以解释如下。
>
> 在处理二分类问题时，写成增广形式后，$w_1$ 的所有样本乘以 $y=1$， $w_2$ 的所有样本乘以 $y=-1$。
>
> 多分类在写成增广形式之后，直接开始迭代计算，没有乘以的操作。
>
> ❓（2）为什么当 $W_1$ 不是最大值时更新呢？
>
> $W_1$ 确定判别函数 $d_1(x)$，$W_2$确定判别函数 $d_2(x)$，$W_3$确定判别函数 $d_3(x)$。
>
> $$w_1 \in \begin{cases}d_1(x) - d_2(x) >0  \\ d_1(x) - d_3(x) > 0& \end{cases}$$
>
> 因此 $x \in w_1$ 时， $W_1$ 是最大值，表示分类正确，不需要更新。
>
> 回答问题（3）也很简单，对于一个分错的样本：
>
> - 一个判别函数可以把它和另外两类分开，更新规则仍然如下：
> - $$\frac{\partial L(w;x,y)}{\partial w} = \begin{cases}0& if &yw^Tx > 0 \\ -yx& if & yw^Tx \leq 0\end{cases}$$
> - 多分类把乘以 $y$ 放到了这里













