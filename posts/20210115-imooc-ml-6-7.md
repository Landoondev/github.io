# 6-7 封装我们的 SGD、sklearn 中的随机梯度下降

## 随机梯度下降

6-6 实现的随机梯度下降存在的问题：每次取得的样本都是随机取的，因此无法保证所有的样本都被取过一遍。

- 合理的 SGD 应该能保证能够看一遍所有的样本。
- 解决的方法是先打乱，再顺序访问这个乱序的样本。

因此参数 `n_iters` 应该表示的是看几遍样本。（即深度学习中常用的 epoch）

## 使用我们自己的 SGD

```python
# 封装 SGD
def fit_sgd(self, X_train, y_train, n_iters=5, t0=5, t1=50):
    """根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型"""
    assert X_train.shape[0] == y_train.shape[0], \
        "the size of X_train must be equal to the size of y_train"
    assert n_iters >= 1 # 保证至少看一遍

    # SGD 求梯度
    def dJ_sgd(theta, X_b_i, y_i):
        return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.

    def sgd(X_b, y, initial_theta, n_iters, t0=5, t1=50):

        def learning_rate(t):
            return t0 / (t + t1)

        theta = initial_theta
        m = len(X_b) # 样本个数

        for cur_iter in range(n_iters):

            indexes = np.random.permutation(m)
            X_b_new = X_b[indexes, :] # Fancy Indexing
            y_new = y[indexes]
            for i in range(m):
                gradient = dJ_sgd(theta, X_b_new[i], y_new[i])
                theta = theta - learning_rate(cur_iter * m + i) * gradient

        return theta

    X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
    initial_theta = np.zeros(X_b.shape[1])
    self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)

    self.intercept_ = self._theta[0]
    self.coef_ = self._theta[1:]

    return self
```

```python
import numpy as np
import matplotlib.pyplot as plt
```

```python
m = 100000

x = np.random.normal(size=m)
X = x.reshape(-1, 1)
y = 4.*x + 3. + np.random.normal(0, 3, size=m)

```

```python
from playML.LinearRegression import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit_sgd(X, y, n_iters=2)
```



```
LinearRegression()
```



```python
lin_reg.coef_
```



```
array([3.9873105])
```



```python
lin_reg.intercept_
```



```
3.000341937459791
```

## 真实的使用我们自己的 SGD

```python
from sklearn import datasets

boston = datasets.load_boston()
X = boston.data
y = boston.target

X = X[y < 50.0]
y = y[y < 50.0]
```

```python
from playML.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, seed=666)
```

```python
# 数据归一化处理
from sklearn.preprocessing import StandardScaler

standardScaler = StandardScaler()
standardScaler.fit(X_train)
X_train_standard = standardScaler.transform(X_train)
X_test_standard = standardScaler.transform(X_test)
```

```python
from playML.LinearRegression import LinearRegression

lin_reg = LinearRegression()
%time lin_reg.fit_sgd(X_train_standard, y_train, n_iters=2)
lin_reg.score(X_test_standard, y_test)
```

```
CPU times: user 6.67 ms, sys: 2.39 ms, total: 9.06 ms
Wall time: 10.6 ms
```





```
0.79111898020977
```



```python
# 增加迭代次数
%time lin_reg.fit_sgd(X_train_standard, y_train, n_iters=50)
lin_reg.score(X_test_standard, y_test)
```

```
CPU times: user 106 ms, sys: 3.9 ms, total: 110 ms
Wall time: 120 ms
```

```
0.8132588958621523
```



```python
# 增加迭代次数
%time lin_reg.fit_sgd(X_train_standard, y_train, n_iters=100)
lin_reg.score(X_test_standard, y_test)
```

```
CPU times: user 188 ms, sys: 2.75 ms, total: 190 ms
Wall time: 192 ms
```



```
0.8131433696273528
```



## scikit-learn 中的 SGD

```python
from sklearn.linear_model import SGDRegressor
```

```python
sgd_reg = SGDRegressor()
# sgd_reg = SGDRegressor(n_iter=100) # 默认值为 1000
%time sgd_reg.fit(X_train_standard, y_train)
sgd_reg.score(X_test_standard, y_test)
```

```
CPU times: user 2.7 ms, sys: 780 µs, total: 3.48 ms
Wall time: 2.52 ms
```



```
0.812244351275594
```



```python
sgd_reg = SGDRegressor(max_iter=10)
%time sgd_reg.fit(X_train_standard, y_train)
sgd_reg.score(X_test_standard, y_test)
```

```
CPU times: user 1.44 ms, sys: 559 µs, total: 2 ms
Wall time: 1.25 ms
```

```
/Users/landonglei/anaconda3/envs/imooc-ml/lib/python3.6/site-packages/sklearn/linear_model/_stochastic_gradient.py:1211: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)
```



```
0.8074665348119214
```

